{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pavank=0\n",
    "class SpikingNetwork:\n",
    "    TAU_MP = 20  # / ms\n",
    "    T_REF = 1  # / ms\n",
    "    ALPHA = 2\n",
    "    ETA_W = 0.002\n",
    "    ETA_TH = ETA_W * 0.1\n",
    "    SIGMA = 0.5\n",
    "    #BETA = 0.0001\n",
    "    #LAMBDA = 0.00000001\n",
    "    \n",
    "\n",
    "    def __init__(self, draw_spike=False):\n",
    "        self.weights = []\n",
    "        self.thresholds = []\n",
    "        self.num_neurons = []\n",
    "        self.v_mps = []\n",
    "        self.kappas = []\n",
    "\n",
    "        self.draw_spike = None\n",
    "        if draw_spike:\n",
    "            self.draw_spike = DrawSpike(network=self)\n",
    "\n",
    "    def add(self, n, kappa=None):\n",
    "        self.num_neurons.append(n)\n",
    "        if len(self.num_neurons) == 1:\n",
    "            if kappa is not None:\n",
    "                print('This kappa is ignored.')\n",
    "            return\n",
    "\n",
    "        self.v_mps.append(np.zeros((n, 1)))\n",
    "\n",
    "        if kappa is None:\n",
    "            kappa = 0\n",
    "        self.kappas.append(kappa)\n",
    "\n",
    "        prev_n = self.num_neurons[len(self.num_neurons) - 2]\n",
    "        root_3_per_m = np.sqrt(3 / prev_n)\n",
    "\n",
    "        self.weights.append(np.random.uniform(-root_3_per_m, root_3_per_m, (n, prev_n)))\n",
    "        self.thresholds.append(np.ones((n, 1)) * SpikingNetwork.ALPHA * root_3_per_m)\n",
    "\n",
    "    # time / ms\n",
    "    def forward(self, x, exposed_time):\n",
    "        self.spikes = [[] for _ in self.num_neurons]\n",
    "        self.x_ks = [np.zeros((num_neuron, 1)) for num_neuron in self.num_neurons[:-1]]\n",
    "        self.a_is = [np.zeros((num_neuron, 1)) for num_neuron in self.num_neurons[1:]]\n",
    "\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        for t in range(exposed_time):\n",
    "            input_spike = x\n",
    "            for i, (v_mp, spike, weight, threshold, x_k, a_i, kappa) in enumerate(zip(\n",
    "                    self.v_mps,\n",
    "                    self.spikes,\n",
    "                    self.weights,\n",
    "                    self.thresholds,\n",
    "                    self.x_ks,\n",
    "                    self.a_is,\n",
    "                    self.kappas\n",
    "            )):\n",
    "                #if not np.any(input_spike):\n",
    "                #    break\n",
    "                spike.append(input_spike)\n",
    "                global pavank\n",
    "                if(pavank==0):\n",
    "                    print(\"data\",input_spike)\n",
    "                    #print(\"X_k.shape\",x_k.shape)\n",
    "                    pavank=1\n",
    "\n",
    "                x_k = SpikingNetwork._calc_x_k(spike, t)\n",
    "                if i > 0:\n",
    "                    self.a_is[i - 1] = x_k\n",
    "\n",
    "                tmp_x_k = x_k.copy()\n",
    "                tmp_x_k[~input_spike] = 0\n",
    "                lateral_inhibition = SpikingNetwork.SIGMA * threshold * kappa * a_i\n",
    "                lateral_inhibition = lateral_inhibition.sum(axis=0) * np.ones(a_i.shape) - lateral_inhibition\n",
    "                v_mp = weight @ tmp_x_k - threshold * a_i + lateral_inhibition\n",
    "\n",
    "                input_spike = np.zeros(v_mp.shape, dtype=bool)\n",
    "                input_spike[v_mp > threshold] = True\n",
    "                v_mp -= (v_mp > threshold) * threshold\n",
    "\n",
    "                self.v_mps[i] = v_mp\n",
    "                self.spikes[i] = spike\n",
    "                self.x_ks[i] = x_k\n",
    "\n",
    "            self.spikes[-1].append(input_spike)\n",
    "            self.a_is[-1] = SpikingNetwork._calc_x_k(self.spikes[-1], t)\n",
    "\n",
    "            if self.draw_spike is not None:\n",
    "                self.draw_spike.update()\n",
    "\n",
    "        self.t = t\n",
    "\n",
    "        for i, _ in enumerate(self.v_mps):\n",
    "            self.v_mps[i] = np.zeros(self.v_mps[i].shape)\n",
    "\n",
    "        #print('forward time: {}'.format(time.time() - start_time))\n",
    "\n",
    "    def backward(self, y):\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        self.x_ks = [SpikingNetwork._calc_x_k(spike, self.t) for spike in self.spikes[:-1]]\n",
    "        self.x_ks = [x_k if x_k.sum() > 0.00001 else np.ones((num_neuron, y.shape[1])) * 10\n",
    "                     for x_k, num_neuron in zip(self.x_ks, self.num_neurons[:-1])]\n",
    "        self.a_is = [SpikingNetwork._calc_x_k(spike, self.t) for spike in self.spikes[1:]]\n",
    "        self.a_is = [a_i if a_i.sum() > 0.00001 else np.ones((num_neuron, y.shape[1])) * 10\n",
    "                     for a_i, num_neuron in zip(self.a_is, self.num_neurons[1:])]\n",
    "        #print('x_ks and a_is time: {}'.format(time.time() - start_time))\n",
    "\n",
    "        sharp_spikes = self._calculate_sharp_spikes()\n",
    "        has_spike_in_output = sharp_spikes.max(axis=0) > 0.0000001\n",
    "        o_is = np.zeros(sharp_spikes.shape)\n",
    "        o_is[:, has_spike_in_output] = sharp_spikes[:, has_spike_in_output] / sharp_spikes[:, has_spike_in_output].max(axis=0)\n",
    "        delta = o_is - y\n",
    "\n",
    "        m_ls = [np.array(sum(spike), bool).sum(axis=0).astype(float) for spike in self.spikes[:-1]]\n",
    "        for m_l in m_ls:\n",
    "            m_l[m_l < 0.00001] = 0.0001\n",
    "\n",
    "        for i, weight, threshold, x_k, a_i, m_l in zip(\n",
    "                reversed(range(len(m_ls))),\n",
    "                reversed(self.weights),\n",
    "                reversed(self.thresholds),\n",
    "                reversed(self.x_ks), reversed(self.a_is),\n",
    "                reversed(m_ls)):\n",
    "            N_l = weight.shape[0]\n",
    "            M_l = weight.shape[1]\n",
    "\n",
    "            '''\n",
    "            for j, this_has_spike_in_output in enumerate(has_spike_in_output):\n",
    "                if not this_has_spike_in_output and np.sum(delta[:, j][:, np.newaxis] @ x_k.T[j][np.newaxis, :]) > 0:\n",
    "                    delta[:, j] *= -1\n",
    "            '''\n",
    "            delta_for_weight = -SpikingNetwork.ETA_W * np.sqrt(N_l / m_l) * delta\n",
    "            if np.all(has_spike_in_output):\n",
    "                delta_weight = delta_for_weight @ x_k.T\n",
    "            else:\n",
    "                import time\n",
    "                s = time.time()\n",
    "                delta_weight = sum(map(lambda x: np.absolute(x[0]) if not x[1] else x[0], [\n",
    "                    (delta_for_weight[:, j][:, np.newaxis] @ x_k.T[j][np.newaxis, :], this_has_spike_in_output)\n",
    "                    for j, this_has_spike_in_output in enumerate(has_spike_in_output)\n",
    "                ]))\n",
    "                #print('delta_weight time: {}'.format(time.time() - s))\n",
    "            delta_threshold = -SpikingNetwork.ETA_TH * np.sqrt(N_l / (m_l * M_l)) * delta * a_i\n",
    "\n",
    "            if i - 1 >= 0:\n",
    "                delta = (1 / self.thresholds[i - 1]) / np.sqrt((1 / m_ls[i - 1]) * np.sum((1 / self.thresholds[i - 1]) ** 2)) * np.sqrt(\n",
    "                    M_l / m_l) * (weight.T @ delta)\n",
    "\n",
    "            weight += delta_weight - 0.0001 * weight\n",
    "            #weight /= np.absolute(weight).sum(axis=1)[:, np.newaxis]\n",
    "            '''\n",
    "            weight_regularization = np.exp(SpikingNetwork.BETA * (np.sum(weight ** 2, axis=1) - 1))[:, np.newaxis]\n",
    "            weight_regularization = SpikingNetwork.BETA * SpikingNetwork.LAMBDA * weight * np.concatenate([\n",
    "            #weight_regularization = 0.5 * SpikingNetwork.LAMBDA * np.concatenate([\n",
    "                weight_regularization for _ in range(weight.shape[1])], axis=1)\n",
    "            #print('weight_regularization: {}'.format(weight_regularization))\n",
    "            #weight -= weight_regularization\n",
    "            weight += delta_weight - weight_regularization\n",
    "            '''\n",
    "            #threshold += delta_threshold.mean(axis=1)[:, np.newaxis]\n",
    "\n",
    "            '''\n",
    "            weight_regularization = np.exp(SpikingNetwork.BETA * (np.sum(weight ** 2, axis=1) - 1))[:, np.newaxis]\n",
    "            weight_regularization = SpikingNetwork.BETA * SpikingNetwork.LAMBDA * weight * np.concatenate([\n",
    "            #weight_regularization = 0.5 * SpikingNetwork.LAMBDA * np.concatenate([\n",
    "                weight_regularization for _ in range(weight.shape[1])], axis=1)\n",
    "            #print('weight_regularization: {}'.format(weight_regularization))\n",
    "            weight -= weight_regularization\n",
    "            '''\n",
    "            #weight += delta_weight - weight_regularization\n",
    "            #threshold += delta_threshold\n",
    "\n",
    "    def infer(self, display_no_spike=False):\n",
    "        sharp_spikes = self._calculate_sharp_spikes() * 100\n",
    "        no_spikes = sharp_spikes.max(axis=0) < 0.000001\n",
    "        if len(no_spikes) > 0 and display_no_spike:\n",
    "            print('No spike... in {}'.format(no_spikes))\n",
    "        max_sharp_spike = np.max(sharp_spikes, axis=0)\n",
    "        return np.exp(sharp_spikes - max_sharp_spike) / np.sum(np.exp(sharp_spikes - max_sharp_spike), axis=0)\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if path is None:\n",
    "            path = os.path.join('./models', '{}.npz'.format(SpikingNetwork._get_latest_model_number() + 1))\n",
    "\n",
    "        content = {\n",
    "            'w{}'.format(i): weight\n",
    "            for i, weight in enumerate(self.weights)\n",
    "        }\n",
    "        content.update({\n",
    "            't{}'.format(i): threshold\n",
    "            for i, threshold in enumerate(self.thresholds)\n",
    "        })\n",
    "        np.savez(path, **content)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        if path is None:\n",
    "            latest_model_number = SpikingNetwork._get_latest_model_number()\n",
    "            if latest_model_number == 0:\n",
    "                raise Exception('There is no numbered model!')\n",
    "            path = os.path.join('./models', '{}.npz'.format(latest_model_number))\n",
    "\n",
    "        content = np.load(path)\n",
    "        self.weights = [content['w{}'.format(i)] for i, _ in enumerate(self.weights)]\n",
    "        self.thresholds = [content['t{}'.format(i)] for i, _ in enumerate(self.thresholds)]\n",
    "\n",
    "    def _calculate_sharp_spikes(self):\n",
    "        return sum(self.spikes[-1])\n",
    "\n",
    "    @classmethod\n",
    "    def _calc_x_k(cls, spike, t):\n",
    "        return sum([np.exp((t_p - t) / cls.TAU_MP) * fire for t_p, fire in enumerate(spike)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_latest_model_number():\n",
    "        path = './models'\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        models = list(filter(lambda y: y is not None, map(lambda x: re.match('[0-9]+.npz', x), os.listdir(path))))\n",
    "        if len(models) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return max(map(lambda x: int(x.group().split('.')[0]), models))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This kappa is ignored.\n",
      "MNIST Loading...OK\n",
      "test data length 100\n",
      "data [[False False False ... False False False]\n",
      " [False False False ...  True False False]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [False  True  True ...  True  True  True]\n",
      " [False False False ...  True False  True]\n",
      " [False False False ... False False False]]\n",
      "No spike... in [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "accuracy 71.3\n",
      "No spike... in [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "accuracy 72.0\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "#from network import SpikingNetwork\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "MAX_DIGIT = 10\n",
    "MINI_BATCH_SIZE = 50\n",
    "\n",
    "\n",
    "pav=0\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "def convert_image(image):\n",
    "    global pav\n",
    "    l=np.array([[x > 0 for j, x in enumerate(y)] for i, y in enumerate(image)]).reshape((64,1))\n",
    "    return l\n",
    "\n",
    "def make_number(number):\n",
    "    zeros = np.zeros((MAX_DIGIT, 1))\n",
    "    zeros[number] = 1\n",
    "    return zeros\n",
    "\n",
    "\n",
    "def main():\n",
    "    if '--help' in sys.argv[1:]:\n",
    "        print('usage: python {} [--load (filename)] [--no-save] [--draw]'.format(sys.argv[0]))\n",
    "        return\n",
    "\n",
    "    network = SpikingNetwork(draw_spike='--draw' in sys.argv[1:])\n",
    "    network.add(64,0)\n",
    "    network.add(30, -0.5)\n",
    "    network.add(MAX_DIGIT, -0.3)\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    print('MNIST Loading...', end='')\n",
    "    #mnist =  pd.read_csv('C:\\\\Users\\\\pavan\\\\Desktop\\\\thesis\\\\SpikingMoYF-master\\\\examples\\\\mnist_train.csv')\n",
    "    #mnist = fetch_openml('mnist_784')\n",
    "    mnist =load_digits()\n",
    "  \n",
    "    #mnist_target=list(map(int,mnist['target']))\n",
    "    #print(map(int,mnist['target'])==4) \n",
    "    \n",
    "    print('OK')\n",
    "\n",
    "    for number in range(0,MAX_DIGIT):\n",
    "        both_data=[]\n",
    "        both_data = [{'x': convert_image(data.reshape(8,8)),'y': make_number(number)} for data in mnist['data'][mnist['target'] == number]]\n",
    "        \"\"\"for data in mnist['data'][mnist['target'] == str(number)]:\n",
    "            print(\"hi\")\n",
    "            l=convert_image(data.reshape(28,28))\n",
    "            m= make_number(number)\n",
    "            both_data=[{'x':l,'y':m}]\n",
    "            print(both_data)\n",
    "        \"\"\"\n",
    "        #convert_image(data.reshape(28,28)\n",
    "        #print(both_data[0]['x'])\n",
    "        #train_data.extend(both_data[:100])\n",
    "        #test_data.extend(both_data[100:105])\n",
    "        #train_data.extend(both_data[:1])\n",
    "        #test_data.extend(both_data[:1])\n",
    "        test_data.extend(both_data[:10])\n",
    "        train_data.extend(both_data[10:])\n",
    "    print(\"test data length\",len(test_data))\n",
    "    #print(\"train data length\",train_data)\n",
    "    train_data\n",
    "    if '--load' in sys.argv[1:]:\n",
    "        print('Loading...', end='')\n",
    "        load_index = sys.argv.index('--load')\n",
    "        if len(sys.argv) > load_index + 1 and os.path.exists(sys.argv[load_index + 1]):\n",
    "            network.load(path=sys.argv[load_index + 1])\n",
    "        else:\n",
    "            network.load()\n",
    "        print('OK')\n",
    "   # print(test_data)\n",
    " \n",
    "\n",
    "    for i in range(1):\n",
    "        network.forward(np.concatenate([data['x'] for data in test_data], axis=1), 64)\n",
    "        answer = np.concatenate([data['y'] for data in test_data], axis=1)\n",
    "        infer = network.infer(display_no_spike=i % 10 == 0)\n",
    "        complete = False\n",
    "        infer_arr=np.array(infer)\n",
    "        answer_arr=np.array(answer)\n",
    "        print(\"accuracy\",np.mean((infer_arr == answer_arr)) * 100)\n",
    "        if np.all(np.absolute(infer - answer) < 0.1):\n",
    "            complete = True\n",
    "        #if i % 5 == 0 or complete:\n",
    "            #print('In {}'.format(i))\n",
    "            #print('answer:\\n{}'.format(answer))\n",
    "            #print('infer:\\n{}'.format(infer#))\n",
    "       # if complete:\n",
    "  #          print('Complete!')\n",
    "            #if '--no-save' not in sys.argv[1:]:\n",
    "                #print('Saving...', end='')#\n",
    "                #network.save()\n",
    "                #print('OK')\n",
    "            #return\n",
    "    \n",
    "        random.shuffle(train_data)\n",
    "        for j in range(0, len(train_data), MINI_BATCH_SIZE):\n",
    "            network.forward(np.concatenate([data['x'] for data in train_data[j:j + MINI_BATCH_SIZE]], axis=1), 64)\n",
    "            network.backward(np.concatenate([data['y'] for data in train_data[j:j + MINI_BATCH_SIZE]], axis=1))\n",
    "    #network.forward(np.concatenate([data['x'] for data in test_data], axis=1), 64)\n",
    "   # for data in train_data[0:1796]:\n",
    "       # pavan_data=np.concatenate([data['x']],axis=1)\n",
    "       # print(\"pavan_data\",pavan_data)\n",
    "    network.forward(np.concatenate([data['x'] for data in test_data], axis=1), 64)\n",
    "    answer = np.concatenate([data['y'] for data in test_data], axis=1)\n",
    "    infer = network.infer(display_no_spike=i % 10 == 0)\n",
    "    complete = False\n",
    "    infer_arr=np.array(infer)\n",
    "    answer_arr=np.array(answer)\n",
    "    print(\"accuracy\",np.mean((infer_arr == answer_arr)) * 100)\n",
    "    \n",
    "    \n",
    "    #l=np.concatenate([data['x'] for data in train_data[0:1600]])\n",
    "    #print(l)\n",
    "    #kk=np.exp((lm-1)/20)*train_data[0]['x']\n",
    "    #print(kk)\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
